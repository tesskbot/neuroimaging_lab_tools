{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Postdoc work\n",
    "\n",
    "\n",
    "## Index\n",
    "\n",
    "* [Goal](#Goal)\n",
    "* [Python code for data wrangling](#Python code for data wrangling)\n",
    "* [R code for linear mixed-effects modeling](#R code for linear mixed-effects modeling)\n",
    "* [Useful tools](#Useful tools)\n",
    "\n",
    "\n",
    "## <a name=\"Goal\"></a>Goal\n",
    "\n",
    "My work aimed to analyze the cognitive data from the BACS cohort to understand how memory and cognition change with healthy aging, and how these changes can be predicted by PIB PET status and other neuroimaging biomarkers, lifestyle factors, and genetic factors. \n",
    "\n",
    "\n",
    "## <a name=\"Python code for data wrangling\"></a>Python code for data wrangling\n",
    "\n",
    "All of my python code exists both as .ipynb files (ipython/jupyter notebook files) and as .py files. The package `datapipeline` and all its functions can be run at the command line or within the ipython notebooks. To import the package contents at the command line, you may first need to point to the data using:\n",
    "\n",
    "```python\n",
    "import sys\n",
    "sys.path.insert(0,'path')\n",
    "```\n",
    "Change the path 'path' if the location of this package has changed.\n",
    "\n",
    "### The datapipeline package:\n",
    "\n",
    "The `datapipeline` package contains code to find, read, and compile longitudinal cognitive data, and also some data from other modalities. The modalities that can currently be gathered are listed in the `gather` subpackage, and more submodules can be added to incoroporate different data modalities. The 'datapipeline_test' folder holds a sample of data used as input ('InData') and the output of all the `datapipeline` scripts below ('OutData') as of August 12th, 2015.\n",
    "\n",
    "The subpackages of `datapipeline` are:\n",
    "\n",
    "#### *gather*\n",
    "\n",
    "The `gathermany` module serves as a front door to the gathering functions. Before running it, you must confirm/change all the paths to point to your data, and designate where you want the data saved. After that, you may run the entire notebook to gather all data modalities and merge them together into summary tables. Cells can also be run individually if you would like to import only a subset of data.\n",
    "\n",
    "The data modalities these scripts now collect are:\n",
    "\n",
    "* **Cognitive testing data** is exported from the database that is maintained by the lab manager. Data exports are files holding the data for each cognitive session, and a file holding basic subject information. This data is processed and factor scores are calculated using factor weights calculated previously. The cognitive data is processed using modules `cogtestdates` and `factoranalysis`.\n",
    "\n",
    "* **PIB data** is extracted using the `pibparams` module. This reads PIB index and subject information from the PIB  directory. \n",
    "\n",
    "* **MRI data** calculated by freesurfer (volumes and thicknesses) is extracted from the aseg.stats file by the code in `mri`. Any number of volumes can be extracted by label name. The extraction is done in place - you simply designate the location where MRI data is stored.\n",
    "\n",
    "* **FDG data** is extracted by the `fdg_metaroi` module. This function is designed to search a parent directory for  FDG data and process all of it.\n",
    "\n",
    "Each of the modules in `gather` (except the script `gathermany`) contains a function named \"*modulename*_run\". This function can be called to gather data of that modality. Most other functions within the modules support the \"*modulename*_run\" function.\n",
    "\n",
    "#### *merge*\n",
    "\n",
    "The `datamerge` module takes the outputs of `gather` and combines them into summary tables. Tables can retain full longitudinal character, or they can be flattened, which compresses the table so that each row represents a single subject.\n",
    "\n",
    "#### *analyze*\n",
    "\n",
    "Following gathering and merging, the data is analyzed. Two notebooks exist to analyze the data in-line and create visualizations. These modules are not intended to run on the command line.\n",
    "\n",
    "The code in `summarize` generates summary statistics and plots variables against each other so that data can be better understood. Data is visualized both cross-sectionally and longitudinally. \n",
    "\n",
    "No files are saved by the code in `analyze`, so these notebooks may be run in their entirety with no negative consequences. The graphs will display inline.\n",
    "\n",
    "#### *tools*\n",
    "\n",
    "The `common_funcs.py` module contains functions perform repeated tasks. Most commonly used is `save_xls_and_pkl` which saves pandas dataframes in both excel and pickle format, appending a timestamp onto the filename.\n",
    "\n",
    "The `radarplot` module creates a radar chart showing the magnitude of numerous variables. It was modified from [this code](http://gist.github.com/sergiobuj/6721187) and is by no means perfect, but it can be modified to meet your needs.\n",
    "\n",
    "\n",
    "## <a name=\"R code for linear mixed-effects modeling\"></a>R code for linear mixed-effects modeling\n",
    "\n",
    "I did linear mixed-effects modeling in R. Linear regression functions are available in some python packages (`stats`, `scikit-learn`), but they do not have built-in functions for mixed-effects modeling. R's `lmer4` package, on the other hand, does. \n",
    "\n",
    "### rscripts\n",
    "\n",
    "#### *mixedmodels.R*\n",
    "\n",
    "This script imports data into R and performs linear regression. Both standard linear regrssion (using `lmList`) and mixed-effects linear modeling (using `lmer`) are done. They are done separately for the different cognitive domains. Look at summarX files to see the model outputs and p-values.\n",
    "\n",
    "#### *ploteffects.R*\n",
    "\n",
    "This function takes the output of `lmer` and generates a bar plot of the fixed effects, in order of magnitude. \n",
    "\n",
    "\n",
    "## <a name=\"Useful tools\"></a>Useful tools\n",
    "\n",
    "### Simultaneously saving .py versions of your .ipynb files\n",
    "\n",
    "I developed all the python code in ipython notebook and saved it concurrently as .py files, which wrote to the same directory as the ipynb files. If you'd like to do this yourself, find your `ipython_notebook_config.py` file by typing `ipython locate` in the bash command line. Then paste this code into the `ipython_notebook_config.py` file: \n",
    "\n",
    "```python\n",
    "import os\n",
    "from subprocess import check_call\n",
    "\n",
    "c = get_config()\n",
    "\n",
    "def post_save(model, os_path, contents_manager):\n",
    "    \"\"\"post-save hook for converting notebooks to .py scripts\"\"\"\n",
    "    if model['type'] != 'notebook':\n",
    "        return # only do this for notebooks\n",
    "    d, fname = os.path.split(os_path)\n",
    "    check_call(['ipython', 'nbconvert', '--to', 'script', fname], cwd=d)\n",
    "\n",
    "c.FileContentsManager.post_save_hook = post_save\n",
    "```\n",
    "\n",
    "These instructions are from this [very helpful Stackoverflow question](https://stackoverflow.com/questions/29329667/ipython-notebook-script-deprecated-how-to-replace-with-post-save-hook).\n",
    "\n",
    "### Get p-values for `lmer`\n",
    "\n",
    "The `lmer` function in R does not produce p-values by default. One of many explanations for why that is the case is [here](https://stat.ethz.ch/pipermail/r-help/2006-May/094765.html), but the gist is that calculating the degrees of freedom is very difficult for multilevel regression. If you still want p-values, you can approximate them using one of a few packages listed [here](http://mindingthebrain.blogspot.com/2014/02/three-ways-to-get-parameter-specific-p.html). I used the Satterthwaite approximation. \n",
    "\n",
    "### z: a command line tool\n",
    "\n",
    "Z uses \"frecency,\" a combination of frequency and recency, to help you navigate quickly at the command line to commonly-used folders. You can learn more about it [here](https://github.com/rupa/z). To install it in your shell, enter \n",
    "\n",
    "```\n",
    "curl -k https://raw.githubusercontent.com/rupa/z/master/z.sh -o ~/z.sh\n",
    "```\n",
    "\n",
    "at the command line. Then, in your `~/.bashrc` file on the cluster (or `~/.bash_profile` if you are installing on a local machine), add this line: \n",
    "\n",
    "```\n",
    ". ~/z.sh\n",
    "```\n",
    "\n",
    "### Fancify your shell\n",
    "\n",
    "If you want to make your bash shell less boring, add some color! In your `~/.bashrc` file (or `~/.bash_profile` if you are installing on a local machine), you can add some extras. The line\n",
    "\n",
    "```\n",
    "PS1=\"\\[\\e[0;35m\\][\\u]@\\h \\w\\n\\[\\033[1;31m\\]$ \\[\\e[0m\\]\"\n",
    "```\n",
    "\n",
    "will add color to your terminal and print your current path with every new line. That is super helpful if you find yourself getting lost and typing `pwd` a lot.\n",
    "\n",
    "Adding aliases for `cd` simplifies long strings of directory changes:\n",
    "\n",
    "```\n",
    "alias ..=\"cd ..\"\n",
    "alias ...=\"cd ../..\"\n",
    "alias ....=\"cd ../../..\"\n",
    "alias .....=\"cd ../../../..\"\n",
    "```\n",
    "\n",
    "Adding an alias for ipython notebook is nice, too:\n",
    "\n",
    "```\n",
    "alias ipy=\"ipython notebook\"\n",
    "```\n",
    "\n",
    "And just for fun:\n",
    "\n",
    "```\n",
    "alias fishie=\"echo '.·¯·.¯·.¸¸.·¯·.¸¸.·¯·.¯·.¸¸.·¯·.¸¸.·¯·.¯·.¸¸.·¯·.¸><(((º>'\"\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
